{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "import pickle\n",
    "from hanziconv import HanziConv\n",
    "\n",
    "np.random.seed(42)\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D, GRU, Bidirectional\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.metrics import categorical_accuracy\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_path = \"./data/chinese_stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringFilterZH():\n",
    "    \"\"\"\n",
    "    中文分词，过滤标点，去除停用词的类\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.stopwords = stopwords_path\n",
    "\n",
    "    def get_stopwords(self):\n",
    "        \"\"\"获得停用词\"\"\"\n",
    "        with open(self.stopwords, 'r', encoding='utf-8') as f:\n",
    "            return set([line.strip() for line in f])\n",
    "\n",
    "    def cut(self, string):\n",
    "        \"\"\"切分词\"\"\"\n",
    "        return list(jieba.cut(string))\n",
    "\n",
    "    def token(self, string):\n",
    "        \"\"\"匹配文字\"\"\"\n",
    "        return re.findall(r'\\w+', string)\n",
    "\n",
    "    def to_simplified(self, string):\n",
    "        \"\"\"繁体转换成简体\"\"\"\n",
    "        return HanziConv.toSimplified(string)\n",
    "\n",
    "    def to_tokens(self, string, use_stopwords=True):\n",
    "        \"\"\"\n",
    "        繁体转换为简体，分词，去除停用词，去除标点保留文字\n",
    "        :param string:\n",
    "        :param use_stopwords:\n",
    "        :return: 词与词空格分隔开\n",
    "        \"\"\"\n",
    "        stopwords = self.get_stopwords()\n",
    "        string = self.to_simplified(string)\n",
    "        tokens = \"\"\n",
    "        for word in self.cut(\"\".join(self.token(string))):\n",
    "            if use_stopwords:\n",
    "                if word not in stopwords:\n",
    "                    tokens += word + \" \"\n",
    "            else:\n",
    "                tokens += word + \" \"\n",
    "        return tokens.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat(labels):\n",
    "    \"\"\"label[-1,-2,1,2]转换成[0,1,0,0,1,0,0,0...]\"\"\"\n",
    "    labels = (np.arange(-2, 2) == labels.reshape(-1, 1)).astype(np.float32)\n",
    "    return labels.flatten() #\n",
    "\n",
    "# 实例化文本清洗\n",
    "string_filter = StringFilterZH()\n",
    "\n",
    "def data_extract(data):\n",
    "    y_label = data.iloc[:, 2:len(data.columns)].values\n",
    "    # 6大类的标签\n",
    "    y_label_l1 = np.stack((np.any(y_label[:, 0:3] != -2, axis=1).astype(np.float32),\n",
    "                               np.any(y_label[:, 3:7] != -2, axis=1).astype(np.float32),\n",
    "                               np.any(y_label[:, 7:10] != -2, axis=1).astype(np.float32),\n",
    "                               np.any(y_label[:, 10:14] != -2, axis=1).astype(np.float32),\n",
    "                               np.any(y_label[:, 14:18] != -2, axis=1).astype(np.float32),\n",
    "                               np.any(y_label[:, 18:20] != -2, axis=1).astype(np.float32))).T\n",
    "    # 20小类的标签\n",
    "    y_label_l2 = np.zeros((y_label.shape[0], 80))\n",
    "    for i in range(y_label_l2.shape[0]):\n",
    "        y_label_l2[i] = reformat(y_label[i])\n",
    "    y = np.hstack((y_label_l2, y_label_l1)) # label 86\n",
    "\n",
    "    # 处理文本内容\n",
    "    x = data[\"content\"].apply(string_filter.to_tokens).values\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessSequence:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 35000 # 词汇表的大小\n",
    "        self.max_len = 300 # 序列最大长度\n",
    "        self.embed_size = 200 # 词嵌入大小\n",
    "        self.embedding_path = r\"./model/word2vec/wiki.zh.model\"\n",
    "\n",
    "    def to_sequence(self, *args):\n",
    "        \"\"\"\n",
    "        返回序列化文本，Embedding词典\n",
    "        :param args: (train， valid， test)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        tokenizer = text.Tokenizer(num_words=self.vocab_size)\n",
    "\n",
    "        corpus = []\n",
    "        for i in args: corpus += list(i)\n",
    "        tokenizer.fit_on_texts(corpus) # 词表\n",
    "\n",
    "        data = []\n",
    "        for i in args:\n",
    "            x_data = tokenizer.texts_to_sequences(i)\n",
    "            x_data = sequence.pad_sequences(x_data, maxlen=self.max_len) # pad sequence\n",
    "            data.append(x_data)\n",
    "\n",
    "        model_wv = KeyedVectors.load(self.embedding_path, mmap='r') # Load 预训练的词向量\n",
    "        word_index = tokenizer.word_index\n",
    "        nb_words = min(self.vocab_size, len(word_index))\n",
    "        embedding_matrix = np.zeros((nb_words, self.embed_size)) # embedding表\n",
    "        for word, i in word_index.items():\n",
    "            if i >= self.vocab_size: continue\n",
    "            try:\n",
    "                embedding_vector = model_wv[word]\n",
    "            except:\n",
    "                embedding_vector = None\n",
    "            if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        del model_wv # 清除内存\n",
    "        _ = gc.collect() # 垃圾回收\n",
    "\n",
    "        data.append(embedding_matrix)\n",
    "        return tuple(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    \"\"\"RocAuc性能评估\"\"\"\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    \"\"\"记录训练时loss acc变化\"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        keys = list(logs.keys())\n",
    "        self.losses['batch'].append(logs.get(keys[0]))\n",
    "        self.accuracy['batch'].append(logs.get(keys[1]))\n",
    "        self.val_loss['batch'].append(logs.get(keys[2]))\n",
    "        self.val_acc['batch'].append(logs.get(keys[3]))\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        keys = list(logs.keys())\n",
    "        self.losses['epoch'].append(logs.get(keys[0]))\n",
    "        self.accuracy['epoch'].append(logs.get(keys[1]))\n",
    "        self.val_loss['epoch'].append(logs.get(keys[2]))\n",
    "        self.val_acc['epoch'].append(logs.get(keys[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    '''自定义attention层，基于Hierarchical Attention Networks for Document Classification'''\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        # Embed_size\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            # c*filters\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        # 对CNN输出进行线性变换\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "        # Tanh\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # Softmax变换\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        # CNN输出加权\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_GRU:\n",
    "    \"\"\"\n",
    "    基于双层双向GRU模型, 6大类\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.hidden_size_1 = 128\n",
    "        self.hidden_size_2 = 64\n",
    "\n",
    "        self.vocab_size = 35000\n",
    "        self.max_len = 300\n",
    "        self.embed_size = 200\n",
    "\n",
    "        self.epochs = 2\n",
    "        self.batch_size = 32\n",
    "        self.model = None\n",
    "\n",
    "    def build_model(self, embedding_matrix):\n",
    "        \"\"\"\n",
    "        建立模型\n",
    "        :param embedding_matrix:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        inp = Input(shape=(self.max_len,))\n",
    "        x = Embedding(self.vocab_size, self.embed_size, weights=[embedding_matrix])(inp)\n",
    "        x = SpatialDropout1D(0.2)(x)\n",
    "\n",
    "        GRU1 = Bidirectional(GRU(self.hidden_size_1, return_sequences=True, recurrent_dropout=0.2,\n",
    "                                 input_shape=(self.max_len, self.embed_size)))(x)\n",
    "        GRU2 = Bidirectional(GRU(self.hidden_size_1, return_sequences=False, recurrent_dropout=0.2,\n",
    "                                 input_shape=(self.max_len, self.hidden_size_1)))(GRU1)\n",
    "\n",
    "        z = Dropout(0.2)(GRU2)\n",
    "\n",
    "        fc = Dense(self.hidden_size_2, activation=\"relu\")(z)\n",
    "\n",
    "        output = Dense(6, activation=\"sigmoid\")(fc)\n",
    "\n",
    "        model = Model(inputs=inp, outputs=output)\n",
    "        model.compile(loss=\"binary_crossentropy\",\n",
    "                      optimizer=\"adam\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "    def train(self, x_train, y_train, x_valid, y_valid, embedding_matrix):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        :param x_train:\n",
    "        :param y_train:\n",
    "        :param x_valid:\n",
    "        :param y_valid:\n",
    "        :param embedding_matrix:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.model = self.build_model(embedding_matrix)\n",
    "        history = LossHistory()\n",
    "        self.model.fit(x_train, y_train[:, 80:], batch_size=self.batch_size,\n",
    "                       epochs=self.epochs, validation_data=(x_valid, y_valid[:, 80:]),\n",
    "                       callbacks=[history])\n",
    "        return history\n",
    "\n",
    "class Model_CNN_Attention:\n",
    "    \"\"\"基础TextCNN模型，20小类\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 35000\n",
    "        self.max_len = 300\n",
    "        self.embed_size = 200\n",
    "\n",
    "        self.num_filters = 32\n",
    "        self.filter_size = [1,2,3,5]\n",
    "\n",
    "        self.epochs = 3\n",
    "        self.batch_size = 64\n",
    "        self.model  = None\n",
    "\n",
    "        self.drop = 0.5\n",
    "\n",
    "    def build_model(self, embedding_matrix):\n",
    "        \"\"\"\n",
    "        建立模型\n",
    "        :param embedding_matrix:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        inp = Input(shape=(self.max_len,))\n",
    "        x = Embedding(self.vocab_size, self.embed_size, weights=[embedding_matrix])(inp)\n",
    "        x = SpatialDropout1D(0.2)(x)\n",
    "        x = Reshape((self.max_len, self.embed_size, 1))(x)\n",
    "\n",
    "        conv_0 = Conv2D(self.num_filters, kernel_size=(self.filter_size[0], self.embed_size), padding=\"valid\",\n",
    "                        kernel_initializer=\"normal\", activation=\"relu\")(x)\n",
    "        conv_1 = Conv2D(self.num_filters, kernel_size=(self.filter_size[1], self.embed_size), padding=\"valid\",\n",
    "                        kernel_initializer=\"normal\", activation=\"relu\")(x)\n",
    "        conv_2 = Conv2D(self.num_filters, kernel_size=(self.filter_size[2], self.embed_size), padding=\"valid\",\n",
    "                        kernel_initializer=\"normal\", activation=\"relu\")(x)\n",
    "        conv_3 = Conv2D(self.num_filters, kernel_size=(self.filter_size[3], self.embed_size), padding=\"valid\",\n",
    "                        kernel_initializer=\"normal\", activation=\"relu\")(x)\n",
    "\n",
    "        z = Concatenate(axis=1)([conv_0, conv_1, conv_2, conv_3]) # (b, c, embed_size, filters)\n",
    "        print(z.ndim)\n",
    "        print(z.shape)\n",
    "        a_shape = (int(z.shape[1]), int(z.shape[-1])) # (embed_size, filters)\n",
    "        z1 = Reshape((-1, a_shape[1]))(z) # (b, c*filters, embed_size)\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(20): # 20小类\n",
    "            z = Attention(a_shape[0])(z1)\n",
    "            z = Dropout(0.1)(z)\n",
    "            z = Dense(64, activation=\"relu\")(z)\n",
    "            out = Dense(4, activation=\"softmax\")(z)\n",
    "            outputs.append(out)\n",
    "        output = Concatenate()(outputs)\n",
    "        model = Model(inputs=inp, outputs=output)\n",
    "\n",
    "        def loss_a(y_true, y_pred):\n",
    "            \"\"\"\n",
    "            计算损失\n",
    "            :param y_true:\n",
    "            :param y_pred:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            loss_sum = 0\n",
    "            for i in range(0, 80, 4):\n",
    "                loss_sum += categorical_crossentropy(y_true[:, i:i+4], y_pred[:, i:i+4])\n",
    "            return loss_sum\n",
    "\n",
    "        def acc(y_true, y_pred):\n",
    "            \"\"\"\n",
    "            计算准确率\n",
    "            :param y_true:\n",
    "            :param y_pred:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            a = 0\n",
    "\n",
    "            for i in range(0, 80, 4):\n",
    "                a += categorical_accuracy(y_true[:, i:i+4], y_pred[:, i:i+4])\n",
    "            return a / 20\n",
    "\n",
    "        model.compile(loss=loss_a,\n",
    "                      optimizer=\"adam\",\n",
    "                      metrics=[acc])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self, x_train, y_train, x_valid, y_valid, embedding_matrix):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        :param x_train:\n",
    "        :param y_train:\n",
    "        :param x_valid:\n",
    "        :param y_valid:\n",
    "        :param embedding_matrix:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.model = self.build_model(embedding_matrix)\n",
    "        history = LossHistory()\n",
    "        self.model.fit(x_train, y_train[:, :80], batch_size=self.batch_size,\n",
    "                       epochs=self.epochs, validation_data=(x_valid, y_valid[:, :80]),\n",
    "                       callbacks=[history])\n",
    "\n",
    "class Model_GRU_Attention:\n",
    "    \"\"\"基础GRU模型\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 35000\n",
    "        self.max_len = 300\n",
    "        self.embed_size = 200\n",
    "\n",
    "        self.epochs = 3\n",
    "        self.batch_size = 32\n",
    "        self.model = None\n",
    "\n",
    "        self.drop = 0.5\n",
    "\n",
    "    def build_model(self, embedding_matrix):\n",
    "        \"\"\"\n",
    "        建立模型\n",
    "        :param embedding_matrix:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        inp = Input(shape=(self.max_len,))\n",
    "        x = Embedding(self.vocab_size, self.embed_size, weights=[embedding_matrix])(inp)\n",
    "        x = SpatialDropout1D(0.2)(x)\n",
    "        x1 = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(20):  # 20小类\n",
    "            z = Attention(self.max_len)(x1)\n",
    "            z = Dropout(0.1)(z)\n",
    "            z = Dense(64, activation=\"relu\")(z)\n",
    "            out = Dense(4, activation=\"softmax\")(z)\n",
    "            outputs.append(out)\n",
    "        output = Concatenate()(outputs)\n",
    "        model = Model(inputs=inp, outputs=output)\n",
    "\n",
    "        def loss_a(y_true, y_pred):\n",
    "            \"\"\"\n",
    "            计算损失\n",
    "            :param y_true:\n",
    "            :param y_pred:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            loss_sum = 0\n",
    "            for i in range(0, 80, 4):\n",
    "                loss_sum += categorical_crossentropy(y_true[:, i:i + 4], y_pred[:, i:i + 4])\n",
    "            return loss_sum\n",
    "\n",
    "        def acc(y_true, y_pred):\n",
    "            \"\"\"\n",
    "            计算准确率\n",
    "            :param y_true:\n",
    "            :param y_pred:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            a = 0\n",
    "\n",
    "            for i in range(0, 80, 4):\n",
    "                a += categorical_accuracy(y_true[:, i:i + 4], y_pred[:, i:i + 4])\n",
    "            return a / 20\n",
    "\n",
    "        model.compile(loss=loss_a,\n",
    "                      optimizer=\"adam\",\n",
    "                      metrics=[acc])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self, x_train, y_train, x_valid, y_valid, embedding_matrix):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        :param x_train:\n",
    "        :param y_train:\n",
    "        :param x_valid:\n",
    "        :param y_valid:\n",
    "        :param embedding_matrix:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.model = self.build_model(embedding_matrix)\n",
    "        history = LossHistory()\n",
    "        self.model.fit(x_train, y_train[:, :80], batch_size=self.batch_size,\n",
    "                       epochs=self.epochs, validation_data=(x_valid, y_valid[:, :80]),\n",
    "                       callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = [\n",
    "        \"./data/train_set.csv\",\n",
    "        \"./data/valid_set.csv\",\n",
    "        \"./data/test_set.csv\"\n",
    "    ]\n",
    "\n",
    "comments_file = \"./data/comments.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\huangm\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.641 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "F:\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel_launcher.py:33: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105000, 300) (105000, 86)\n"
     ]
    }
   ],
   "source": [
    "# 文本处理，标签onehot编码\n",
    "x_train, y_train = data_extract(pd.read_csv(data_files[0]))\n",
    "x_valid, y_valid = data_extract(pd.read_csv(data_files[1]))\n",
    "x_test, _ = data_extract(pd.read_csv(data_files[2]))\n",
    "\n",
    "# 序列化文本\n",
    "process_sequence = ProcessSequence()\n",
    "x_train, x_valid, x_test, embedding_matrix = process_sequence.to_sequence(x_train, x_valid, x_test)\n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    f = open(comments_file, \"wb\")\n",
    "    save = {\n",
    "        \"x_train\": x_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"x_valid\": x_valid,\n",
    "        \"y_valid\": y_valid,\n",
    "        \"x_test\": x_test,\n",
    "        \"embedding_matrix\": embedding_matrix\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print(\"Unable to save data to\", comments_file, ':', e)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 300) (5000, 86)\n"
     ]
    }
   ],
   "source": [
    "with open(comments_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    x_train = save['x_train']\n",
    "    y_train = save['y_train']\n",
    "    x_valid = save['x_valid']\n",
    "    y_valid = save['y_valid']\n",
    "    x_test = save['x_test']\n",
    "    embedding_matrix = save['embedding_matrix']\n",
    "    del save  # hint to help gc free up memory\n",
    "\n",
    "x_train, y_train = x_train[:5000], y_train[:5000]\n",
    "x_valid, y_valid = x_valid[:500], y_valid[:500]\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 500 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 86s 17ms/step - loss: 223049361046310527959040.0000 - accuracy: 0.5985 - val_loss: 0.6143 - val_accuracy: 0.7367\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 77s 15ms/step - loss: 512128662990521676857344.0000 - accuracy: 0.6963 - val_loss: 0.5779 - val_accuracy: 0.7423\n"
     ]
    }
   ],
   "source": [
    "model_1 = Model_GRU()\n",
    "model_1.train(x_train, y_train, x_valid, y_valid, embedding_matrix)\n",
    "test_pred_1 = model_1.model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4975001 , 0.5717782 , 0.61088794, 0.57704043, 0.6897824 ,\n",
       "        0.6499232 ],\n",
       "       [0.44916743, 0.5495137 , 0.63975155, 0.6393579 , 0.5828957 ,\n",
       "        0.64245105],\n",
       "       [0.5169344 , 0.64462763, 0.5782553 , 0.4784829 , 0.69152486,\n",
       "        0.63911515],\n",
       "       [0.53878707, 0.6269418 , 0.6248623 , 0.54119116, 0.64035976,\n",
       "        0.70528233],\n",
       "       [0.42829216, 0.68456703, 0.5508314 , 0.53215647, 0.62774944,\n",
       "        0.6962418 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 500 samples\n",
      "Epoch 1/3\n",
      "5000/5000 [==============================] - 95s 19ms/step - loss: 16.4644 - acc: 0.6904 - val_loss: 14.5875 - val_acc: 0.7189\n",
      "Epoch 2/3\n",
      "5000/5000 [==============================] - 88s 18ms/step - loss: 13.5258 - acc: 0.7395 - val_loss: 12.9192 - val_acc: 0.7573\n",
      "Epoch 3/3\n",
      "5000/5000 [==============================] - 90s 18ms/step - loss: 11.9316 - acc: 0.7695 - val_loss: 11.9623 - val_acc: 0.7748\n"
     ]
    }
   ],
   "source": [
    "# model_2 = Model_CNN_Attention()\n",
    "model_2 = Model_GRU_Attention()\n",
    "model_2.train(x_train, y_train, x_valid, y_valid, embedding_matrix)\n",
    "test_pred_2 = model_2.model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_env]",
   "language": "python",
   "name": "conda-env-tensorflow_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
