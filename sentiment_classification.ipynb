{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gou2f5Ej41kq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "faf2ac5a-ca71-4764-e346-2e19578ec255"
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spuXhzEs6lUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive  -o nonempty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJCz_Xdf1An5",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_q25H-249vj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1387e35a-2e12-4510-ad81-8ce8f5b68ece"
      },
      "source": [
        "!/opt/bin/nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: /opt/bin/nvidia-smi: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pFSSJnR5HrI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7ebd5d73-ec88-4eb1-f5af-3781d68d651f"
      },
      "source": [
        "import tensorflow\n",
        "tensorflow.test.is_gpu_available"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function tensorflow.python.framework.test_util.is_gpu_available>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jugWEWEK1IO3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "1d27af22-7047-40fd-ff2b-906c41d17058"
      },
      "source": [
        "!pip install hanziconv"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hanziconv\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/71/b89cb63077fd807fe31cf7c016a06e7e579a289d8a37aa24a30282d02dd2/hanziconv-0.3.2.tar.gz (276kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 18.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 81kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 225kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 235kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 245kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 256kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 266kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286kB 3.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: hanziconv\n",
            "  Building wheel for hanziconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hanziconv: filename=hanziconv-0.3.2-py2.py3-none-any.whl size=23215 sha256=0eb75e69edca2d2b0b165ca8fecc251dfb0ae9fbbd4c367bfe38b98162e2fcbc\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/d8/3c/c39898fa9c9ce6e34b0ab4c6604892462d440c743715c94054\n",
            "Successfully built hanziconv\n",
            "Installing collected packages: hanziconv\n",
            "Successfully installed hanziconv-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZUfgTuR6fkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import jieba\n",
        "import re\n",
        "import pickle\n",
        "from hanziconv import HanziConv\n",
        "\n",
        "np.random.seed(42)\n",
        "import gc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Dense, Conv2D\n",
        "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D, GRU, Bidirectional\n",
        "from keras import initializers, regularizers, constraints\n",
        "from keras.engine.topology import Layer\n",
        "from keras import backend as K\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.metrics import categorical_accuracy\n",
        "\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import Callback\n",
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYxS8thg1CMZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e7307354-e115-416b-f894-cdf63be64ed1"
      },
      "source": [
        "import os\n",
        "path = \"/content/drive/My Drive\"\n",
        "os.chdir(path) # 到指定路径\n",
        "os.listdir(path) "
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP', '.ipynb_checkpoints']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gz4ma4l2moA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bcab287e-0318-478e-a4fe-1fa42cce772a"
      },
      "source": [
        "os.getcwd() # 查看当前工作目录"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaCA6rxi296O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords_path = \"./NLP/data/stopwords.txt\""
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBqGvzfe3d9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StringFilterZH():\n",
        "  \"\"\"\n",
        "  中文分词，过滤标点，去除停用词的类\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.stopwords = stopwords_path\n",
        "\n",
        "  def get_stopwords(self):\n",
        "    \"\"\"获得停用词\"\"\"\n",
        "    with open(self.stopwords, 'r', encoding='utf-8') as f:\n",
        "      return set([line.strip() for line in f])\n",
        "\n",
        "  def cut(self, string):\n",
        "    \"\"\"切分词\"\"\"\n",
        "    return list(jieba.cut(string))\n",
        "\n",
        "  def token(self, string):\n",
        "    \"\"\"匹配文字\"\"\"\n",
        "    return re.findall(r'\\w+', string)\n",
        "\n",
        "  def to_simplified(self, string):\n",
        "    \"\"\"繁体转换成简体\"\"\"\n",
        "    return HanziConv.toSimplified(string)\n",
        "\n",
        "  def to_tokens(self, string, use_stopwords=True):\n",
        "    \"\"\"\n",
        "    繁体转换为简体，分词，去除停用词，去除标点保留文字\n",
        "    :param string:\n",
        "    :param use_stopwords:\n",
        "    :return: 词与词空格分隔开\n",
        "    \"\"\"\n",
        "    stopwords = self.get_stopwords()\n",
        "    string = self.to_simplified(string)\n",
        "    tokens = \"\"\n",
        "    for word in self.cut(\"\".join(self.token(string))):\n",
        "        if use_stopwords:\n",
        "            if word not in stopwords:\n",
        "                tokens += word + \" \"\n",
        "        else:\n",
        "            tokens += word + \" \"\n",
        "    return tokens.strip()\n",
        "\n",
        "def reformat(labels):\n",
        "  \"\"\"label[-1,-2,1,2]转换成[0,1,0,0,1,0,0,0...]\"\"\"\n",
        "  labels = (np.arange(-2, 2) == labels.reshape(-1, 1)).astype(np.float32)\n",
        "  return labels.flatten() #\n",
        "\n",
        "# 实例化文本清洗\n",
        "string_filter = StringFilterZH()\n",
        "\n",
        "def data_extract(data):\n",
        "  y_label = data.iloc[:, 2:len(data.columns)].values\n",
        "  # 6大类的标签\n",
        "  y_label_l1 = np.stack((np.any(y_label[:, 0:3] != -2, axis=1).astype(np.float32),\n",
        "              np.any(y_label[:, 3:7] != -2, axis=1).astype(np.float32),\n",
        "              np.any(y_label[:, 7:10] != -2, axis=1).astype(np.float32),\n",
        "              np.any(y_label[:, 10:14] != -2, axis=1).astype(np.float32),\n",
        "              np.any(y_label[:, 14:18] != -2, axis=1).astype(np.float32),\n",
        "              np.any(y_label[:, 18:20] != -2, axis=1).astype(np.float32))).T\n",
        "  # 20小类的标签\n",
        "  y_label_l2 = np.zeros((y_label.shape[0], 80))\n",
        "  for i in range(y_label_l2.shape[0]):\n",
        "    y_label_l2[i] = reformat(y_label[i])\n",
        "  y = np.hstack((y_label_l2, y_label_l1)) # label 86\n",
        "\n",
        "  # 处理文本内容\n",
        "  x = data[\"content\"].apply(string_filter.to_tokens).values\n",
        "\n",
        "  return x, y"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNA6LY6J3ez9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ProcessSequence:\n",
        "  def __init__(self):\n",
        "    self.vocab_size = 35000 # 词汇表的大小\n",
        "    self.max_len = 300 # 序列最大长度\n",
        "    self.embed_size = 200 # 词嵌入大小\n",
        "    self.embedding_path = r\"./NLP/model/wiki.zh.model\"\n",
        "\n",
        "  def to_sequence(self, *args):\n",
        "    \"\"\"\n",
        "    返回序列化文本，Embedding词典\n",
        "    :param args: (train， valid， test)\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    tokenizer = text.Tokenizer(num_words=self.vocab_size)\n",
        "\n",
        "    corpus = []\n",
        "    for i in args: corpus += list(i)\n",
        "    tokenizer.fit_on_texts(corpus) # 词表\n",
        "\n",
        "    data = []\n",
        "    for i in args:\n",
        "        x_data = tokenizer.texts_to_sequences(i)\n",
        "        x_data = sequence.pad_sequences(x_data, maxlen=self.max_len) # pad sequence\n",
        "        data.append(x_data)\n",
        "\n",
        "    model_wv = KeyedVectors.load(self.embedding_path, mmap='r') # Load 预训练的词向量\n",
        "    word_index = tokenizer.word_index\n",
        "    nb_words = min(self.vocab_size, len(word_index))\n",
        "    embedding_matrix = np.zeros((nb_words, self.embed_size)) # embedding表\n",
        "    for word, i in word_index.items():\n",
        "        if i >= self.vocab_size: continue\n",
        "        try:\n",
        "            embedding_vector = model_wv[word]\n",
        "        except:\n",
        "            embedding_vector = None\n",
        "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "    del model_wv # 清除内存\n",
        "    _ = gc.collect() # 垃圾回收\n",
        "\n",
        "    data.append(embedding_matrix)\n",
        "    return tuple(data)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1tJDxDA4Fct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RocAucEvaluation(Callback):\n",
        "  \"\"\"RocAuc性能评估\"\"\"\n",
        "  def __init__(self, validation_data=(), interval=1):\n",
        "    super(Callback, self).__init__()\n",
        "\n",
        "    self.interval = interval\n",
        "    self.X_val, self.y_val = validation_data\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if epoch % self.interval == 0:\n",
        "      y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "      score = roc_auc_score(self.y_val, y_pred)\n",
        "      print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
        "\n",
        "class LossHistory(Callback):\n",
        "  \"\"\"记录训练时loss acc变化\"\"\"\n",
        "  def on_train_begin(self, logs={}):\n",
        "    self.losses = {'batch':[], 'epoch':[]}\n",
        "    self.accuracy = {'batch':[], 'epoch':[]}\n",
        "    self.val_loss = {'batch':[], 'epoch':[]}\n",
        "    self.val_acc = {'batch':[], 'epoch':[]}\n",
        "  def on_batch_end(self, batch, logs={}):\n",
        "    keys = list(logs.keys())\n",
        "    self.losses['batch'].append(logs.get(keys[0]))\n",
        "    self.accuracy['batch'].append(logs.get(keys[1]))\n",
        "    self.val_loss['batch'].append(logs.get(keys[2]))\n",
        "    self.val_acc['batch'].append(logs.get(keys[3]))\n",
        "  def on_epoch_end(self, batch, logs={}):\n",
        "    keys = list(logs.keys())\n",
        "    self.losses['epoch'].append(logs.get(keys[0]))\n",
        "    self.accuracy['epoch'].append(logs.get(keys[1]))\n",
        "    self.val_loss['epoch'].append(logs.get(keys[2]))\n",
        "    self.val_acc['epoch'].append(logs.get(keys[3]))"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfkBpzd_3unt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(Layer):\n",
        "  \"\"\"自定义attention层，基于Hierarchical Attention Networks for Document Classification\"\"\"\n",
        "  def __init__(self, step_dim,\n",
        "          W_regularizer=None, b_regularizer=None,\n",
        "          W_constraint=None, b_constraint=None,\n",
        "          bias=True, **kwargs):\n",
        "    self.supports_masking = True\n",
        "    self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "    self.W_regularizer = regularizers.get(W_regularizer)\n",
        "    self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "    self.W_constraint = constraints.get(W_constraint)\n",
        "    self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "    self.bias = bias\n",
        "    self.step_dim = step_dim\n",
        "    self.features_dim = 0\n",
        "    super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    assert len(input_shape) == 3\n",
        "    # Embed_size\n",
        "    self.W = self.add_weight(shape=(input_shape[-1],),\n",
        "                              initializer=self.init,\n",
        "                              name='{}_W'.format(self.name),\n",
        "                              regularizer=self.W_regularizer,\n",
        "                              constraint=self.W_constraint)\n",
        "    self.features_dim = input_shape[-1]\n",
        "\n",
        "    if self.bias:\n",
        "        # c*filters\n",
        "        self.b = self.add_weight(shape=(input_shape[1],),\n",
        "                                  initializer='zero',\n",
        "                                  name='{}_b'.format(self.name),\n",
        "                                  regularizer=self.b_regularizer,\n",
        "                                  constraint=self.b_constraint)\n",
        "    else:\n",
        "        self.b = None\n",
        "    self.built = True\n",
        "\n",
        "  def compute_mask(self, input, input_mask=None):\n",
        "    return None\n",
        "\n",
        "  def call(self, x, mask=None):\n",
        "    features_dim = self.features_dim\n",
        "    step_dim = self.step_dim\n",
        "    # 对CNN输出进行线性变换\n",
        "    eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
        "                    K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "\n",
        "    if self.bias:\n",
        "        eij += self.b\n",
        "    # Tanh\n",
        "    eij = K.tanh(eij)\n",
        "\n",
        "    a = K.exp(eij)\n",
        "\n",
        "    if mask is not None:\n",
        "        a *= K.cast(mask, K.floatx())\n",
        "    # Softmax变换\n",
        "    a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "    a = K.expand_dims(a)\n",
        "    # CNN输出加权\n",
        "    weighted_input = x * a\n",
        "    return K.sum(weighted_input, axis=1)\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return input_shape[0],  self.features_dim\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDhPUCWR5ruG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model_GRU:\n",
        "  \"\"\"\n",
        "  基于双层双向GRU模型, 6大类\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.hidden_size_1 = 128\n",
        "    self.hidden_size_2 = 64\n",
        "\n",
        "    self.vocab_size = 35000\n",
        "    self.max_len = 300\n",
        "    self.embed_size = 200\n",
        "\n",
        "    self.epochs = 2\n",
        "    self.batch_size = 128\n",
        "    self.model = None\n",
        "\n",
        "  def build_model(self, embedding_matrix):\n",
        "    \"\"\"\n",
        "    建立模型\n",
        "    :param embedding_matrix:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    inp = Input(shape=(self.max_len,))\n",
        "    x = Embedding(self.vocab_size, self.embed_size, weights=[embedding_matrix])(inp)\n",
        "    x = SpatialDropout1D(0.2)(x)\n",
        "\n",
        "    GRU1 = Bidirectional(GRU(self.hidden_size_1, return_sequences=True, recurrent_dropout=0.2,\n",
        "                              input_shape=(self.max_len, self.embed_size)))(x)\n",
        "    GRU2 = Bidirectional(GRU(self.hidden_size_1, return_sequences=False, recurrent_dropout=0.2,\n",
        "                              input_shape=(self.max_len, self.hidden_size_1)))(GRU1)\n",
        "\n",
        "    z = Dropout(0.2)(GRU2)\n",
        "\n",
        "    fc = Dense(self.hidden_size_2, activation=\"relu\")(z)\n",
        "\n",
        "    output = Dense(6, activation=\"sigmoid\")(fc)\n",
        "\n",
        "    model = Model(inputs=inp, outputs=output)\n",
        "    model.compile(loss=\"binary_crossentropy\",\n",
        "                  optimizer=\"adam\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "  def train(self, x_train, y_train, x_valid, y_valid, embedding_matrix):\n",
        "    \"\"\"\n",
        "    训练模型\n",
        "    :param x_train:\n",
        "    :param y_train:\n",
        "    :param x_valid:\n",
        "    :param y_valid:\n",
        "    :param embedding_matrix:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    self.model = self.build_model(embedding_matrix)\n",
        "    history = LossHistory()\n",
        "    self.model.fit(x_train, y_train[:, 80:], batch_size=self.batch_size,\n",
        "                    epochs=self.epochs, validation_data=(x_valid, y_valid[:, 80:]),\n",
        "                    callbacks=[history])\n",
        "    return history"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPhkkhec56v1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model_CNN_Attention:\n",
        "  \"\"\"基础TextCNN模型，20小类\"\"\"\n",
        "  def __init__(self):\n",
        "    self.vocab_size = 35000\n",
        "    self.max_len = 300\n",
        "    self.embed_size = 200\n",
        "\n",
        "    self.num_filters = 32\n",
        "    self.filter_size = [1,2,3,5]\n",
        "\n",
        "    self.epochs = 3\n",
        "    self.batch_size = 256\n",
        "    self.model  = None\n",
        "\n",
        "    self.drop = 0.5\n",
        "\n",
        "  def build_model(self, embedding_matrix):\n",
        "    \"\"\"\n",
        "    建立模型\n",
        "    :param embedding_matrix:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    inp = Input(shape=(self.max_len,))\n",
        "    x = Embedding(self.vocab_size, self.embed_size, weights=[embedding_matrix])(inp)\n",
        "    x = SpatialDropout1D(0.2)(x)\n",
        "    x = Reshape((self.max_len, self.embed_size, 1))(x)\n",
        "\n",
        "    conv_0 = Conv2D(self.num_filters, kernel_size=(self.filter_size[0], self.embed_size), padding=\"valid\",\n",
        "                    kernel_initializer=\"normal\", activation=\"relu\")(x)\n",
        "    conv_1 = Conv2D(self.num_filters, kernel_size=(self.filter_size[1], self.embed_size), padding=\"valid\",\n",
        "                    kernel_initializer=\"normal\", activation=\"relu\")(x)\n",
        "    conv_2 = Conv2D(self.num_filters, kernel_size=(self.filter_size[2], self.embed_size), padding=\"valid\",\n",
        "                    kernel_initializer=\"normal\", activation=\"relu\")(x)\n",
        "    conv_3 = Conv2D(self.num_filters, kernel_size=(self.filter_size[3], self.embed_size), padding=\"valid\",\n",
        "                    kernel_initializer=\"normal\", activation=\"relu\")(x)\n",
        "\n",
        "    z = Concatenate(axis=1)([conv_0, conv_1, conv_2, conv_3]) # (b, c, embed_size, filters)\n",
        "    a_shape = (int(z.shape[1]), int(z.shape[-1])) # (embed_size, filters)\n",
        "    z1 = Reshape((-1, a_shape[1]))(z) # (b, c*filters, embed_size)\n",
        "\n",
        "    outputs = []\n",
        "    for i in range(20): # 20小类\n",
        "      z = Attention(a_shape[0])(z1)\n",
        "      z = Dropout(0.1)(z)\n",
        "      z = Dense(64, activation=\"relu\")(z)\n",
        "      out = Dense(4, activation=\"softmax\")(z)\n",
        "      outputs.append(out)\n",
        "    output = Concatenate()(outputs)\n",
        "    model = Model(inputs=inp, outputs=output)\n",
        "\n",
        "    def loss_a(y_true, y_pred):\n",
        "      \"\"\"\n",
        "      计算损失\n",
        "      :param y_true:\n",
        "      :param y_pred:\n",
        "      :return:\n",
        "      \"\"\"\n",
        "      loss_sum = 0\n",
        "      for i in range(0, 80, 4):\n",
        "        loss_sum += categorical_crossentropy(y_true[:, i:i+4], y_pred[:, i:i+4])\n",
        "      return loss_sum\n",
        "\n",
        "    def acc(y_true, y_pred):\n",
        "      \"\"\"\n",
        "      计算准确率\n",
        "      :param y_true:\n",
        "      :param y_pred:\n",
        "      :return:\n",
        "      \"\"\"\n",
        "      a = 0\n",
        "\n",
        "      for i in range(0, 80, 4):\n",
        "        a += categorical_accuracy(y_true[:, i:i+4], y_pred[:, i:i+4])\n",
        "      return a / 20\n",
        "\n",
        "    model.compile(loss=loss_a,\n",
        "            optimizer=\"adam\",\n",
        "            metrics=[acc])\n",
        "\n",
        "    return model\n",
        "\n",
        "  def train(self, x_train, y_train, x_valid, y_valid, embedding_matrix):\n",
        "    \"\"\"\n",
        "    训练模型\n",
        "    :param x_train:\n",
        "    :param y_train:\n",
        "    :param x_valid:\n",
        "    :param y_valid:\n",
        "    :param embedding_matrix:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    self.model = self.build_model(embedding_matrix)\n",
        "    history = LossHistory()\n",
        "    self.model.fit(x_train, y_train[:, :80], batch_size=self.batch_size,\n",
        "            epochs=self.epochs, validation_data=(x_valid, y_valid[:, :80]),\n",
        "            callbacks=[history])\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RyuCB_w6FaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model_GRU_Attention:\n",
        "  \"\"\"基础GRU模型\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.vocab_size = 35000\n",
        "    self.max_len = 300\n",
        "    self.embed_size = 200\n",
        "\n",
        "    self.epochs = 3\n",
        "    self.batch_size = 128\n",
        "    self.model = None\n",
        "\n",
        "    self.drop = 0.5\n",
        "\n",
        "  def build_model(self, embedding_matrix):\n",
        "    \"\"\"\n",
        "    建立模型\n",
        "    :param embedding_matrix:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    inp = Input(shape=(self.max_len,))\n",
        "    x = Embedding(self.vocab_size, self.embed_size, weights=[embedding_matrix])(inp)\n",
        "    x = SpatialDropout1D(0.2)(x)\n",
        "    x1 = Bidirectional(GRU(128, return_sequences=True))(x)\n",
        "\n",
        "    outputs = []\n",
        "    for i in range(20):  # 20小类\n",
        "      z = Attention(self.max_len)(x1)\n",
        "      z = Dropout(0.1)(z)\n",
        "      z = Dense(64, activation=\"relu\")(z)\n",
        "      out = Dense(4, activation=\"softmax\")(z)\n",
        "      outputs.append(out)\n",
        "    output = Concatenate()(outputs)\n",
        "    model = Model(inputs=inp, outputs=output)\n",
        "\n",
        "    def loss_a(y_true, y_pred):\n",
        "      \"\"\"\n",
        "      计算损失\n",
        "      :param y_true:\n",
        "      :param y_pred:\n",
        "      :return:\n",
        "      \"\"\"\n",
        "      loss_sum = 0\n",
        "      for i in range(0, 80, 4):\n",
        "        loss_sum += categorical_crossentropy(y_true[:, i:i + 4], y_pred[:, i:i + 4])\n",
        "      return loss_sum\n",
        "\n",
        "    def acc(y_true, y_pred):\n",
        "      \"\"\"\n",
        "      计算准确率\n",
        "      :param y_true:\n",
        "      :param y_pred:\n",
        "      :return:\n",
        "      \"\"\"\n",
        "      a = 0\n",
        "\n",
        "      for i in range(0, 80, 4):\n",
        "        a += categorical_accuracy(y_true[:, i:i + 4], y_pred[:, i:i + 4])\n",
        "      return a / 20\n",
        "\n",
        "    model.compile(loss=loss_a,\n",
        "            optimizer=\"adam\",\n",
        "            metrics=[acc])\n",
        "\n",
        "    return model\n",
        "\n",
        "  def train(self, x_train, y_train, x_valid, y_valid, embedding_matrix):\n",
        "    \"\"\"\n",
        "    训练模型\n",
        "    :param x_train:\n",
        "    :param y_train:\n",
        "    :param x_valid:\n",
        "    :param y_valid:\n",
        "    :param embedding_matrix:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    self.model = self.build_model(embedding_matrix)\n",
        "    history = LossHistory()\n",
        "    self.model.fit(x_train, y_train[:, :80], batch_size=self.batch_size,\n",
        "            epochs=self.epochs, validation_data=(x_valid, y_valid[:, :80]),\n",
        "            callbacks=[history])"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unb6uzFsAtNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_files = [\n",
        "        \"./NLP/data/train_set.csv\",\n",
        "        \"./NLP/data/valid_set.csv\",\n",
        "        \"./NLP/data/test_set.csv\"\n",
        "    ]\n",
        "comments_file = \"./NLP/data/comments.pickle\""
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz0CS9qh7FWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 文本处理，标签onehot编码\n",
        "# x_train, y_train = data_extract(pd.read_csv(data_files[0]))\n",
        "# x_valid, y_valid = data_extract(pd.read_csv(data_files[1]))\n",
        "# x_test, _ = data_extract(pd.read_csv(data_files[2]))\n",
        "\n",
        "# # 序列化文本\n",
        "# process_sequence = ProcessSequence()\n",
        "# x_train, x_valid, x_test, embedding_matrix = process_sequence.to_sequence(x_train, x_valid, x_test)\n",
        "\n",
        "# print(x_train.shape, y_train.shape)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g3LQe498aed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try:\n",
        "#   f = open(comments_file, \"wb\")\n",
        "#   save = {\n",
        "#       \"x_train\": x_train,\n",
        "#       \"y_train\": y_train,\n",
        "#       \"x_valid\": x_valid,\n",
        "#       \"y_valid\": y_valid,\n",
        "#       \"x_test\": x_test,\n",
        "#       \"embedding_matrix\": embedding_matrix\n",
        "#   }\n",
        "#   pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
        "# except Exception as e:\n",
        "#   print(\"Unable to save data to\", comments_file, ':', e)\n",
        "    \n",
        "# f.close()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTmJv-hb8bnL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cb6ab0a9-d67d-490f-a2c9-ad4322529367"
      },
      "source": [
        "with open(comments_file, 'rb') as f:\n",
        "  save = pickle.load(f)\n",
        "  x_train = save['x_train']\n",
        "  y_train = save['y_train']\n",
        "  x_valid = save['x_valid']\n",
        "  y_valid = save['y_valid']\n",
        "  x_test = save['x_test']\n",
        "  embedding_matrix = save['embedding_matrix']\n",
        "  del save  # hint to help gc free up memory\n",
        "\n",
        "x_train, y_train = x_train[:15000], y_train[:15000]\n",
        "x_valid, y_valid = x_valid[:1500], y_valid[:1500]\n",
        "print(x_train.shape, y_train.shape)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15000, 300) (15000, 86)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlMkKWxI8pds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "1beed831-6558-464a-b7a4-56ad222fd01e"
      },
      "source": [
        "model_1 = Model_GRU()\n",
        "model_1.train(x_train, y_train, x_valid, y_valid, embedding_matrix)\n",
        "test_pred_1 = model_1.model.predict(x_test)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 1500 samples\n",
            "Epoch 1/2\n",
            "15000/15000 [==============================] - 470s 31ms/step - loss: 169888413514649443500032.0000 - accuracy: 0.6525 - val_loss: 0.6108 - val_accuracy: 0.7402\n",
            "Epoch 2/2\n",
            "15000/15000 [==============================] - 469s 31ms/step - loss: 241582971672142972715008.0000 - accuracy: 0.7270 - val_loss: 0.5625 - val_accuracy: 0.7504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARvQJ4YtBzbz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "630f9928-7850-4355-e041-96a5133bf2cb"
      },
      "source": [
        "# model_2 = Model_CNN_Attention()\n",
        "model_2 = Model_GRU_Attention()\n",
        "model_2.train(x_train, y_train, x_valid, y_valid, embedding_matrix)\n",
        "test_pred_2 = model_2.model.predict(x_test)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 1500 samples\n",
            "Epoch 1/3\n",
            "15000/15000 [==============================] - 239s 16ms/step - loss: 16.6029 - acc: 0.6853 - val_loss: 14.2916 - val_acc: 0.7290\n",
            "Epoch 2/3\n",
            "15000/15000 [==============================] - 232s 15ms/step - loss: 13.2737 - acc: 0.7453 - val_loss: 12.2235 - val_acc: 0.7662\n",
            "Epoch 3/3\n",
            "15000/15000 [==============================] - 233s 16ms/step - loss: 11.6774 - acc: 0.7774 - val_loss: 11.1067 - val_acc: 0.7898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P-ppKfVRNu3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "2f8248b9-e23c-43d6-8e1d-331fc144b909"
      },
      "source": [
        "model_2 = Model_CNN_Attention()\n",
        "model_2.train(x_train, y_train, x_valid, y_valid, embedding_matrix)\n",
        "test_pred_2 = model_2.model.predict(x_test)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 1500 samples\n",
            "Epoch 1/3\n",
            "15000/15000 [==============================] - 23s 2ms/step - loss: 18.6825 - acc: 0.6552 - val_loss: 16.9016 - val_acc: 0.6871\n",
            "Epoch 2/3\n",
            "15000/15000 [==============================] - 12s 828us/step - loss: 16.5516 - acc: 0.6884 - val_loss: 15.9569 - val_acc: 0.6958\n",
            "Epoch 3/3\n",
            "15000/15000 [==============================] - 12s 828us/step - loss: 15.8339 - acc: 0.6968 - val_loss: 15.3656 - val_acc: 0.7070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clxql8fNN-8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_file = \"./NLP/result.pickle\""
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYQfAwUHA3L6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4435a382-528d-4e9e-f4a8-52eed12b5d56"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15000, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBDrsNeSPTPo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6dc469f2-4247-4dd3-876a-b677885cc57e"
      },
      "source": [
        "data = pd.read_csv(data_files[2])\n",
        "data.head()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>content</th>\n",
              "      <th>location_traffic_convenience</th>\n",
              "      <th>location_distance_from_business_district</th>\n",
              "      <th>location_easy_to_find</th>\n",
              "      <th>service_wait_time</th>\n",
              "      <th>service_waiters_attitude</th>\n",
              "      <th>service_parking_convenience</th>\n",
              "      <th>service_serving_speed</th>\n",
              "      <th>price_level</th>\n",
              "      <th>price_cost_effective</th>\n",
              "      <th>price_discount</th>\n",
              "      <th>environment_decoration</th>\n",
              "      <th>environment_noise</th>\n",
              "      <th>environment_space</th>\n",
              "      <th>environment_cleaness</th>\n",
              "      <th>dish_portion</th>\n",
              "      <th>dish_taste</th>\n",
              "      <th>dish_look</th>\n",
              "      <th>dish_recommendation</th>\n",
              "      <th>others_overall_experience</th>\n",
              "      <th>others_willing_to_consume_again</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>\"我想说他们家的优惠活动好持久啊，我预售的时候买的券，前两天心血来潮去吃的活动还在继续\\n首...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>\"终于开到心心念念的LAB loft。第一次来就随便点也一些～【香辣虾意面】蛮辣的，但其实一...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>\"地理位置好，交通方便，就在124车站对面交通方便，很好，我晚上7点多去买的了，已经没有什么...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>\"运气很好，抽中了大众点评的霸王餐。这家主题餐厅心仪已久了，种种原因一直未能成行，没想到抽中...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>\"幸运随点评团体验霸王餐，心情好~蜀九香刚进驻泉州不久，招牌大名气响，以至于刚到店门口的我被...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ... others_willing_to_consume_again\n",
              "0   0  ...                             NaN\n",
              "1   1  ...                             NaN\n",
              "2   2  ...                             NaN\n",
              "3   3  ...                             NaN\n",
              "4   4  ...                             NaN\n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm5FISl4PiIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = data[\"content\"]"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaqZNtvvNRnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  f = open(result_file, \"wb\")\n",
        "  save = {\n",
        "      \"text\": text,\n",
        "      \"pred\": test_pred_2,\n",
        "  }\n",
        "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
        "except Exception as e:\n",
        "  print(\"Unable to save data to\", result_file, ':', e)\n",
        "    \n",
        "f.close()"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQOmBghDQeDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}